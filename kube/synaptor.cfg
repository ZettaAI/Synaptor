[Volumes]
# Voxelwise descriptor volume to segment
descriptor = [DESCRIPTOR VOLUME PATH]

# Output segmentation
output = [OUTPUT PATH]

# Temporary output for intermediate results
tempoutput = [TEMP OUTPUT PATH]

# "Base" segmentation for overlaps and assignment.
#  Likely the morphological segmentation
baseseg = [BASE SEG PATH]

# EM image volume
image = [IMAGE PATH]


[Dimensions]
# Voxel resolution (X, Y, Z)
voxelres = 8, 8, 40

# Lower bound of the bounding box
startcoord = 0, 0, 0

# Size of the bounding box in each dimension
volshape = 1024, 1024, 1024

# Size of a single chunk in each dimension
chunkshape = 256, 256, 256

# Size of a storage block in cloud storage for 
# the output & tempoutput volumes
blockshape = 64, 64, 16

# Patch size for assignment inference (ignored if not run)
patchshape = 80, 80, 18


[Parameters]
# Connected Components Threshold
ccthresh = 0.5

# Object Size Threshold
szthresh = 100

# Dust Threshold
dustthresh = 0

# How much to split up some tasks for merging results across chunks.
# This should be set to 1 for almost all workloads
nummergetasks = 1

# Merging distance for segments assigned to the same partners
# (units should match `voxelres` above)
mergethresh = 0

# Batch size for AVAN inference (only required for synapse assignment)
batchsz = 1


[Workflow]
# The type of workflow to be run. Determines when some tasks need to happen
# {Segmentation, Segmentation+Assignment}
workflowtype = Segmentation

# The kind of storage backend to use. Database is currently required for
# parallel merging operations
# {Database, File}
workspacetype = File

# URL for accessing the task message queue
queueurl = [QUEUE URL]

# SQL Alchemy connection string, location of a file containing this string,
# or STORAGE_FROM_FILE if using a kubernetes secret.
# This is ignored if workspacetype = File
connectionstr = STORAGE_FROM_FILE

# A directory for storing intermediate data outside of the database.
# This is required for either workspacetype
storagedir = [STORAGE DIR]
